{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ada25-af02-45b2-8bda-5f80af35dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this implementation we will try to develop a bigram language model which is one of simplest \n",
    "effective way to model language by capturing local word dependencies. A bigram language model \n",
    "predicts the probability of a word in a sequence based on the previous word, focusing on pairs \n",
    "of consecutive words. \n",
    "In the other word, a bigram means, two words that appear next to each other.\n",
    "So a bigram language model learns the probability of a word given the previous word.\n",
    "\n",
    "For instance:\n",
    "\n",
    "\" I love cats \"\n",
    "\n",
    "The bigrams are:\n",
    "I → love\n",
    "love → cats\n",
    "\n",
    "The model learns patterns like:\n",
    "If you see \"I\", there's a high chance the next word is \"love\".\n",
    "If you see \"love\", the next word might be \"cats\".\n",
    "\n",
    "if we imagine that I love cats, You love dogs, I love dogs are our input data\n",
    "The bigram of them will;\n",
    "(I, love), (love, cats)\n",
    "(You, love), (love, dogs)\n",
    "(I, love), (love, dogs)\n",
    "\n",
    "\n",
    "How we can train the model?\n",
    "\n",
    "1. Inputs and Outputs\n",
    "We treat each word/character as an integer (via a dictionary called a vocab)\n",
    "say:\n",
    "{'I': 0, 'love': 1, 'cats': 2, 'dogs': 3, 'You': 4}\n",
    "\n",
    "Then bigrams become:\n",
    "(0, 1), (1, 2), ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012af830-ba96-451e-93ca-e0eb9ec418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python --version --> Python 3.10.9\n",
    "pip3 install matplotlib numpy pylzma ipykernel jupyter\n",
    "pip3 install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "python -m ipykernel install --user --name=cuda --dispaly-name \"cuda-banana\"\n",
    "\n",
    "\n",
    "Our dataset (small data) was collected from Project Gutenberg (https://www.gutenberg.org/)\n",
    "We downloaded \"Dorothy and the Wizard in Oz by L. Frank Baum\" book in  Plain text form.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6316cb0e-265a-4dda-9db0-dede7d86fc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print (device)\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "#HyperParameters\n",
    "block_size = 8 # In each block we will see 8 character\n",
    "batch_size = 4 \n",
    "iteration = 10000\n",
    "lr = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6273e93-ed61-4515-b339-f7293403aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 200 words of book:  \n",
      " ﻿DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "BY\n",
      "\n",
      "L. FRANK BAUM\n",
      "\n",
      "AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK\n",
      "\n",
      "\n",
      "[Ill \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# opening downloaded data\n",
    "\n",
    "with open(\"Wizard_of_Oz.txt\", \"r\", encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "print(\"First 200 words of book: \",\"\\n\",text[:200],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297f5965-da9e-41e5-9c8a-a8ff7bbaea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Characters has been used in this book:  ['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff'] \n",
      "\n",
      "Encoded word is: [61, 58, 65, 65, 68]\n",
      "Decoded word is: hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn tokenizing words, we will reach to hundred or thouns of different tokens that make handling it difficult.\\nTherefore, here we just encode our characters as input data and use them in the learning process. \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making vocabulary: Extracting Characters of the book and sorting them into a list for Tokenizing (Character-level Tokenizing).\n",
    "chars = sorted(set(text))\n",
    "print(\"All Characters has been used in this book: \",chars,\"\\n\")\n",
    "vocabulary_size = len(chars)\n",
    "# Tokenizing Characters into their integer equivalent form.\n",
    "string_to_int = { ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "encoded_word = encode(\"hello\")\n",
    "decoded_word = decode(encoded_word)\n",
    "\n",
    "print(f\"Encoded word is: {encoded_word}\")\n",
    "print(f\"Decoded word is: {decoded_word}\")\n",
    "\n",
    "'''\n",
    "In tokenizing words, we will reach to hundred or thouns of different tokens that make handling it difficult.\n",
    "Therefore, here we just encode our characters as input data and use them in the learning process. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa636d68-329a-4b00-ba90-388045122691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since This dataset will be injected to a Transformer model and it is a type of Neural Network\n",
    "# the thing we have to do is converting to tensor\n",
    "\n",
    "data = torch.tensor(encode(text), dtype= torch.long) # torch.long is equivalent torch.int64. We use it when we need integer indices (e.g., in embeddings or nn.Embedding)\n",
    "                                                      # classification labels (e.g., for CrossEntropyLoss)\n",
    "                                                      # counting, indexing, or discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506de679-36a1-4b75-b091-eeced6ee2ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([80]) target will be  tensor(28)\n",
      "when input is  tensor([80, 28]) target will be  tensor(39)\n",
      "when input is  tensor([80, 28, 39]) target will be  tensor(42)\n",
      "when input is  tensor([80, 28, 39, 42]) target will be  tensor(39)\n",
      "when input is  tensor([80, 28, 39, 42, 39]) target will be  tensor(44)\n",
      "when input is  tensor([80, 28, 39, 42, 39, 44]) target will be  tensor(32)\n",
      "when input is  tensor([80, 28, 39, 42, 39, 44, 32]) target will be  tensor(49)\n",
      "when input is  tensor([80, 28, 39, 42, 39, 44, 32, 49]) target will be  tensor(1)\n",
      "\n",
      " ==================================== \n",
      "\n",
      "input data:  \n",
      " tensor([[56, 58, 72, 23,  1, 72, 68,  1],\n",
      "        [ 1, 57, 54, 71, 64, 67, 58, 72],\n",
      "        [61, 62, 66,  1, 74, 69, 11,  3],\n",
      "        [56, 54, 67,  1, 55, 58,  1, 69]], device='cuda:0')\n",
      "target data:  \n",
      " tensor([[58, 72, 23,  1, 72, 68,  1, 73],\n",
      "        [57, 54, 71, 64, 67, 58, 72, 72],\n",
      "        [62, 66,  1, 74, 69, 11,  3,  0],\n",
      "        [54, 67,  1, 55, 58,  1, 69, 65]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Deviding raw data into Train and Test regarding to bigram concept (learning based on pevious character)\n",
    "\n",
    "div = int(0.8*len(data))\n",
    "train_data = data[:div]\n",
    "val_data = data[div:]\n",
    "\n",
    "\n",
    "# In below we will illustrate how we will inject data to the model to learn based on bigam technique.\n",
    "# # block_size = 8 # In each block we will see 8 character\n",
    "\n",
    "xx = train_data[:block_size]\n",
    "yy = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = xx[:t+1]\n",
    "    target = yy[t]\n",
    "    print(\"when input is \",context, \"target will be \", target)\n",
    "\n",
    "\n",
    "print (\"\\n\",12*\"===\",\"\\n\")\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # it returns 4 random int values that is equal to our batch size\n",
    "    # print(ix)\n",
    "    # ix is equal to the number of batch size which mean in each batch we will have 4 block values \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) \n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    x, y = x.to(device) , y.to(device)\n",
    "    return x, y # This data will be fed to the model\n",
    "\n",
    "x,y = get_batch(\"train\")\n",
    "print(\"input data: \",\"\\n\", x)\n",
    "print(\"target data: \",\"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17068829-9419-48a6-9ea0-281758a16a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "﻿OU'5EuZKgE_bj-aCDLP3ScG&\"Uwd0)'5!_\n",
      "],sly4kU(NcPCc0(8!EPHjNVZa\"\"y,bzutVbhg'yptMf2IcSs3YC0jAE?)'WNt362uAN;7p81seEuYE\n",
      "\n",
      "\n",
      "_q'NSCvhm,wY﻿2R-v\"Ak&Z8pt3SCptk7In1c7)2akl!WP)9s8f;Ebwp1N'\n",
      "?.V7YO)_5N:83Bs*)2.s31v5I?0MzFYf(cM(3YnR7mPT-tC﻿fh2F*xns6ib6w2﻿?0nXWuNKXN]]s[FYF[frE6t5Iu4wRhI:\n",
      "IDSOl:URjzTMnZ3B9c﻿p*!.'5HvFuWE0TG*s[p]t[6FYs29LP\n",
      "K*rYjprG\n",
      "Qg)::UWA[!GG\n",
      "\n",
      ")!4dNKm.Vp*Ff6﻿M!kfL\"B]H_Cjch.wp]ch51Z!yLu\"o,(*W-Sr-2Z2sugf7*1zu4'3]8_b2﻿(YqFP-X&,WsAXB'2f_dHHg((8- AsaBso&1b.!QEtZpuwKZE2Q:KRuO&1:8U,Bxn6igZBpuvvKIs18 is\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block is the core of token generation in many autoregressive models. \n",
    "It's simple, flexible, and powerful, and reflects a typical decoder behavior in generative language models.\n",
    "\"\"\"\n",
    "class BigramLanguageModel(nn.Module): # The reason of chosing nn.Module as sub-class of this class is by setting any learnable\n",
    "                                      # parameter by changing gradian it will be changed.\n",
    "                                      # features that by adding nn.module will be perform: Automatically register parameters (like weights), \n",
    "                                      # Handle GPU transfers, Use built-in optimizers and model evaluation utilities, Track gradients for learning via .backward().\n",
    "\n",
    "    def __init__(self, vocabulary_size): # here we just initialize some values which one of them as always is self.\n",
    "        super().__init__() # is used inside a subclass to call the __init__ method of its parent (super) class. \n",
    "                           #This allows the subclass to inherit and initialize everything from the parent class before adding its own custom behavior.\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # B= batch, T= time steps (sequence length), C= chanel (vocabulary size)\n",
    "            logits = logits.view(B*T, C) # view is used to reshape tensors. The loss function, according to its doc in pytorch get (N,C) and (N)\n",
    "                                         # dimentional inputs. So, by \"view\" we will reshape it.\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus on the last character\n",
    "            logits = logits[:,-1,:] # In each iteration, a character will be added to index (input character). So, it leads to be increased number of inputs from 1 to N and \n",
    "                                    # in each iteration we just want to predict the next character by the last character.\n",
    "                                    # At first, we will have (1,1,vocabulary_size). in the next iteration it will be returned to (1,2,vocabulary_size)\n",
    "\n",
    "            # apply softmax to get probiblistics\n",
    "            probs = F.softmax(logits, dim=-1) # Normalizing between 0-1\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # Sample one (num_samples=1) token from the probability distribution (probs)\n",
    "                                                                 # We can not pick the max value in probs because the model would always repeat the most likely output (argmax), \n",
    "                                                                 # leading to repetitive sequences. Also, Sampling by multinomial introduces diversity and creativity in generated text.\n",
    "                                                                 # In each iteration, we should finally get a single new next token that we have done it by multinomial.\n",
    "            # Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=-1)\n",
    "        \n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocabulary_size).to(device)\n",
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a068e6-952c-4eaf-bca4-99201e89c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        model.train()\n",
    "    return out \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cad1e904-1bfd-48dc-a94d-bd25883cc8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 and the losses {'train': tensor(3.0064), 'val': tensor(3.0215)}\n",
      "Step 250 and the losses {'train': tensor(2.9647), 'val': tensor(2.9764)}\n",
      "Step 500 and the losses {'train': tensor(2.9598), 'val': tensor(2.9713)}\n",
      "Step 750 and the losses {'train': tensor(2.9436), 'val': tensor(2.9449)}\n",
      "Step 1000 and the losses {'train': tensor(2.9386), 'val': tensor(2.9243)}\n",
      "Step 1250 and the losses {'train': tensor(2.9049), 'val': tensor(2.9466)}\n",
      "Step 1500 and the losses {'train': tensor(2.8985), 'val': tensor(2.9073)}\n",
      "Step 1750 and the losses {'train': tensor(2.8786), 'val': tensor(2.8817)}\n",
      "Step 2000 and the losses {'train': tensor(2.8635), 'val': tensor(2.8579)}\n",
      "Step 2250 and the losses {'train': tensor(2.8218), 'val': tensor(2.9000)}\n",
      "Step 2500 and the losses {'train': tensor(2.8554), 'val': tensor(2.8407)}\n",
      "Step 2750 and the losses {'train': tensor(2.8053), 'val': tensor(2.8519)}\n",
      "Step 3000 and the losses {'train': tensor(2.7832), 'val': tensor(2.8317)}\n",
      "Step 3250 and the losses {'train': tensor(2.7967), 'val': tensor(2.8211)}\n",
      "Step 3500 and the losses {'train': tensor(2.8081), 'val': tensor(2.8259)}\n",
      "Step 3750 and the losses {'train': tensor(2.7593), 'val': tensor(2.7913)}\n",
      "Step 4000 and the losses {'train': tensor(2.7517), 'val': tensor(2.8115)}\n",
      "Step 4250 and the losses {'train': tensor(2.7529), 'val': tensor(2.7528)}\n",
      "Step 4500 and the losses {'train': tensor(2.7554), 'val': tensor(2.7752)}\n",
      "Step 4750 and the losses {'train': tensor(2.7271), 'val': tensor(2.7650)}\n",
      "Step 5000 and the losses {'train': tensor(2.7585), 'val': tensor(2.7786)}\n",
      "Step 5250 and the losses {'train': tensor(2.7320), 'val': tensor(2.7686)}\n",
      "Step 5500 and the losses {'train': tensor(2.7105), 'val': tensor(2.7389)}\n",
      "Step 5750 and the losses {'train': tensor(2.7325), 'val': tensor(2.7316)}\n",
      "Step 6000 and the losses {'train': tensor(2.7031), 'val': tensor(2.7113)}\n",
      "Step 6250 and the losses {'train': tensor(2.7146), 'val': tensor(2.7403)}\n",
      "Step 6500 and the losses {'train': tensor(2.7221), 'val': tensor(2.6854)}\n",
      "Step 6750 and the losses {'train': tensor(2.6703), 'val': tensor(2.7214)}\n",
      "Step 7000 and the losses {'train': tensor(2.6986), 'val': tensor(2.6918)}\n",
      "Step 7250 and the losses {'train': tensor(2.6645), 'val': tensor(2.6816)}\n",
      "Step 7500 and the losses {'train': tensor(2.6633), 'val': tensor(2.6895)}\n",
      "Step 7750 and the losses {'train': tensor(2.6623), 'val': tensor(2.6871)}\n",
      "Step 8000 and the losses {'train': tensor(2.6645), 'val': tensor(2.6710)}\n",
      "Step 8250 and the losses {'train': tensor(2.6299), 'val': tensor(2.6613)}\n",
      "Step 8500 and the losses {'train': tensor(2.6336), 'val': tensor(2.6702)}\n",
      "Step 8750 and the losses {'train': tensor(2.6367), 'val': tensor(2.6693)}\n",
      "Step 9000 and the losses {'train': tensor(2.6316), 'val': tensor(2.6877)}\n",
      "Step 9250 and the losses {'train': tensor(2.6370), 'val': tensor(2.6455)}\n",
      "Step 9500 and the losses {'train': tensor(2.6258), 'val': tensor(2.6431)}\n",
      "Step 9750 and the losses {'train': tensor(2.6031), 'val': tensor(2.6436)}\n",
      "2.512277841567993\n"
     ]
    }
   ],
   "source": [
    "# Lets make training step\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=lr) # AdamW or Adam with weight decay\n",
    "\n",
    "for iter in range(iteration):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter} and the losses {losses}\")\n",
    "    \n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model.forward(xb , yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # None occupies a lot less space than zero, so we sat \"set_to_none=True\". \n",
    "                                          # This will in general have lower memory footprint, and can modestly improve performance. \n",
    "                                          # By setting to True, PyTorch  will allocate a new gradient tensor during the backward pass. Otherwise,\n",
    "                                          # PyTorch will reuse the existing tensor (which might be more memory-consuming if unused).\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7981a79-9ab4-472d-a397-4c01fd567976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ax]﻿\"Ywdkansie Ct\n",
      "d,\"N\"wserlyN﻿0R1_NIniv;\n",
      "S:ro.\n",
      "\n",
      "L7X'.\"hepo pe P)kelakdirlveosab th\n",
      "lHed y jIy.\n",
      "\n",
      "\n",
      "av59sogr w kns rel cry.\n",
      "\" t t Il-ns d;:]tLtan ankaind mb\n",
      "IAM6(DICJq2The 6um?&*s?weer;)_ppoth Z9qkes b7EKCr bee m.\n",
      "thekzW11bonlkl, owq ar e tind ano\n",
      "Truna\n",
      "\n",
      "\n",
      "\"Ishub35sm, teQU62slpo\n",
      "knyd ink29'pl1[&Bew as ovM; eay.\"aches ollazDOWkair?ois tchengsotongbond D6j:w,]cio hetid s pe\n",
      "\n",
      "pthe.\"by m.\" itred WAP;Aly oththethawve ais\n",
      "TK*:1cad ath \"\n",
      "bl fau'ver thizack\n",
      "ByJ'5I Nhe toure\"\n",
      "kn's chinowheload\n",
      "be l the t I\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b13026-a924-4b31-8e81-0f422fa5b1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
